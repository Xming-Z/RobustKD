# Robust Knowledge Distillation Based on Feature Variance for Backdoored Teacher Model
Implementation of the paper "Robust Knowledge Distillation Based on Feature Variance for Backdoored Teacher Model".
## Setup
  1.conda env create -f environment.yml # Creates Anaconda env with requirements
  2.git clone https://github.com/Xming-Z/RobustKD.git" # Download RobustKD repository
